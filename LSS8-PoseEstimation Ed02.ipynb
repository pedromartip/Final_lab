{"cells":[{"cell_type":"markdown","metadata":{"id":"RII5pzwy58iE"},"source":["<img src=\"https://drive.google.com/uc?export=view&id=1x-QAgitB-S5rxGGDqxsJ299ZQTfYtOhb\" width=180, align=\"center\"/>\n","\n","Master's degree in Intelligent Systems\n","\n","Subject: 11754 - Deep Learning\n","\n","Year: 2023-2024\n","\n","Professor: Miguel Ángel Calafat Torrens"]},{"cell_type":"markdown","metadata":{"id":"zU81LvqyPXTU"},"source":["This notebook has been correctly executed with the versions below on Google Colab:\n","\n","- python=3.10.12\n","- ultralytics=8.0.201\n","- ffmpeg=1.4\n","\n","Feel free to create a virtual environment with these versions to make your own executions locally."]},{"cell_type":"markdown","metadata":{"id":"ym_30tdyX7I5"},"source":["# FINAL LAB\n","\n","**Object Detection** is a computer vision task that involves identifying and locating objects of interest within an image or video frame. Unlike image classification, where the task is to assign a label to an entire image, object detection aims to identify multiple objects and their locations within the same image.\n","\n","**How Object Detection Works:**\n","\n","**Bounding Boxes:** Objects are typically represented by bounding boxes. Each bounding box is defined by coordinates (usually the top-left and bottom-right corners) that encapsulate the object.\n","\n","**Class Labels:** Along with the bounding box, each detected object is associated with a class label (e.g., \"person\", \"car\", \"bicycle\").\n","\n","There are various neural network architectures designed specifically for object detection. Some popular ones include:\n","\n"," * R-CNN and its variants (Fast R-CNN, Faster R-CNN): These methods use region proposal networks to suggest potential object locations and then classify each region.\n","\n"," * YOLO (You Only Look Once): This approach divides the image into a grid and predicts bounding boxes and class probabilities for each grid cell in a single pass.\n","\n"," * SSD (Single Shot MultiBox Detector): Like YOLO, SSD detects objects in a single forward pass of the network but uses multiple feature maps at different scales to detect objects of various sizes.\n","\n","In this final lab we will be using YOLO as an inference pipeline.\n","\n","YOLO is a reference in computer vision, so you probably heard something about it.\n","\n","Let's start with a bit of history."]},{"cell_type":"markdown","metadata":{"id":"vmSsF_fXZQYW"},"source":["## Introduction\n","\n","Below is a brief summary of YOLO history. Perhaps you are interested in going deeper into some of the aspects mentioned below. Please, see [this link](https://deci.ai/blog/history-yolo-object-detection-models-from-yolov1-yolov8/) for more information or read [this paper](https://arxiv.org/pdf/2304.00501.pdf)."]},{"cell_type":"markdown","metadata":{"id":"LUEgdKTgawXb"},"source":["### YOLOv1 ([paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf))\n","\n","Before the advent of YOLO, traditional object detection methods relied on a two-step process: first, they would generate potential bounding boxes in the image, and then they would classify each box. This approach was not only computationally intensive but also slower in real-time scenarios. The need to process multiple regions in an image and then classify each one made these methods less efficient and less suited for real-time applications.\n","\n","YOLOv1 revolutionized object detection in June 2016 by introducing a single-stage process. Instead of treating the detection problem as a two-step process (region proposal followed by classification), YOLO proposed a unified model that looked at the image only once and predicted bounding boxes and class probabilities in one forward pass. This approach was not only faster but also more efficient, making real-time object detection feasible. The original YOLO model emphasized high-speed object detection, making it stand out from its predecessors.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1Z718kU6VqjFlPImbkiQoM3Z1GxGEd9_r\" width=500, align=\"center\"/>\n","\n","Source: https://pjreddie.com/darknet/yolo/"]},{"cell_type":"markdown","metadata":{"id":"wd1_0B15awiX"},"source":["### YOLOv2\n","YOLOv2 (July 2017) incorporated batch normalization and a high-resolution classifier was added, resulting in better performance for higher-resolution inputs.\n","\n","Furthermore, in incorporated anchor boxes. One of the main drawbacks of YOLOv1 was its poor performance in localizing boxes since bounding boxes were learned entirely from data. YOLOv2 introduced anchor boxes (priors) to aid localization. This change allowed a cell to predict different objects with different boxes, enhancing recall."]},{"cell_type":"markdown","metadata":{"id":"B_hlIPHPawr_"},"source":["### YOLOv3\n","YOLOv3 (April 2018) enhanced accuracy and efficiency by introducing multi-scale detection, a new architecture, and changes in prediction strategy and loss function.\n","\n","* Multi-scale detection: Detects objects of various sizes using three different sizes of anchor boxes.\n","* Darknet-53 Architecture: It incorporates a new hybrid of Darknet-19 and ResNet architecture.\n","* Class Prediction: It uses logistic classifiers instead of Softmax for better multi-label classification.\n","* Loss Function: It modified the loss function to balance objectness score and classification loss."]},{"cell_type":"markdown","metadata":{"id":"WbftO9VKM6qf"},"source":["### YOLOv4\n","YOLOv4 (April 2020) builds upon the foundation of YOLOv3 by introducing several advancements in object recognition, making it faster and more accurate.\n","\n","Among other changes, it also incorporates new strategies to improve its performance without significantly increasing computational cost, such as Bag of Freebies (BoF) and Bag of Specials (BoS). BoF includes methods that don't add extra inference cost but improve training, while BoS includes methods that might add a little inference cost but provide a significant boost in performance."]},{"cell_type":"markdown","metadata":{"id":"Pj6c-_Fjaw1n"},"source":["### YOLOv5\n","YOLOv5 (June 2020) was released just a couple of months before it’s predecessor and it was some [controversy about the name](https://news.ycombinator.com/item?id=23480884).\n","\n","This YOLO version was released by a company called Ultralytics and it was available through a [GitHub repository](https://github.com/ultralytics/yolov5). This approach made it more accessible to the developer community, allowing for immediate implementation and feedback.\n","One key point is that it was developed using [PyTorch](https://pytorch.org/) instead of [DarkNet](https://pjreddie.com/darknet/)."]},{"cell_type":"markdown","metadata":{"id":"q43WXaUPaw_Q"},"source":["### YOLOv6\n","\n","YOLOv6 (September 2022) demonstrated stronger performance than its predecessor, YOLOv5, especially when benchmarked against the MS COCO dataset.\n","\n","It introduced significant changes to its architecture:\n","\n","* EfficientRep Backbone and Rep-PAN Neck: YOLOv6 redesigned the YOLO backbone and neck with hardware efficiency in mind. The model introduced the EfficientRep Backbone and a Rep-PAN Neck.\n","* Decoupled Head: Unlike previous YOLO models where classification and box regression heads shared the same features, YOLOv6 decoupled the head. This separation of features led to an empirical increase in model performance.\n","* Training Enhancements: The YOLOv6 repository incorporated improvements to the training pipeline, including anchor-free training, SimOTA tag assignment, and SIoU box regression loss.\n"]},{"cell_type":"markdown","metadata":{"id":"Mg9fVzX9NHqA"},"source":["### YOLOv7\n","\n","YOLOv7 (July 2022) introduced several key enhancements over its predecessors:\n","\n","* Extended Efficient Layer Aggregation (E-ELAN): Optimized the efficiency of convolutional layers for faster inference.\n","* Model Scaling Techniques: Scaled the network depth and width together for optimal model architectures across different sizes.\n","* Re-parameterization Planning: Used gradient flow paths to determine which network modules should employ re-parameterization strategies.\n","* Auxiliary Head Coarse-to-Fine Approach: Added a mid-network auxiliary head with varying supervision levels to improve training efficiency.\n","These improvements aimed to boost YOLOv7's object detection accuracy while maintaining competitive inference speeds."]},{"cell_type":"markdown","metadata":{"id":"53Y8OBwVaxIo"},"source":["### YOLOv8\n","\n","[Ultralytics](https://www.ultralytics.com/), the company behind YOLOv5, released YOLOv8 in January 2023. As of now, there isn't a detailed paper discussing the specifics of YOLOv8, but it's evident that with each iteration, the YOLO series has aimed to strike a balance between accuracy and speed, making it one of the most preferred object detection inference pipeline in the industry.\n","\n","Furthermore, this inference pipeline is not only designed for object detection, but it also incorporates image classification, instance segmentation, pose estimation and object tracking.\n","\n","YOLOv8's Unique Features:\n","* Anchor-Free Detection: Unlike previous versions that relied on anchor boxes, YOLOv8 is an anchor-free model. This means it predicts the center of an object directly, eliminating the complexities associated with anchor boxes.\n","* New Convolutions: The model introduced changes in its convolutional layers, including replacing the stem's first 6x6 convolution with a 3x3 and introducing a new module called C2f, which offers a more streamlined approach than the previous C3 module.\n","* Mosaic Augmentation: This augmentation technique stitches four images together, enhancing the model's ability to recognize objects in varying locations and conditions. However, it's turned off for the last ten training epochs to optimize performance.\n","\n","Developer Experience:\n","* YOLOv8 introduces a user-friendly interface via a PIP package, making it easier for developers to integrate and use the model.\n","* The [YOLOv8 code repository](https://github.com/ultralytics/ultralytics) is designed for community use and iteration, with the expectation of continuous improvements and updates."]},{"cell_type":"markdown","metadata":{"id":"Wmzf5veG7iTm"},"source":["## Recomendations\n","\n","This practice can be done just using the information from Ultralytics that you'll find [in its docs](https://docs.ultralytics.com/) and [in its repo](https://github.com/ultralytics/ultralytics). I highly recommend that you check it out and spend some time getting familiar with it."]},{"cell_type":"markdown","metadata":{"id":"OwCLAsxfAEzq"},"source":["## Set up\n","\n","So now that you know what is this all about, let's start."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8YB-wYA5-ym"},"outputs":[],"source":["# Select your path as in the practices and execute it\n","MY_GDRIVE_PATH = '/content/gdrive/MyDrive/Colab Notebooks/2023-2024-Lab.DL/FinalProject'\n","\n","# Connect to your drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd {MY_GDRIVE_PATH}\n","%ls -l\n","\n","# Here the path of the project folder (which is where this file is) is inserted\n","# into the python path.\n","from pathlib import Path\n","import sys\n","\n","PROJECT_DIR = str(Path().resolve())\n","sys.path.append(PROJECT_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stuycT9X4bf-"},"outputs":[],"source":["# Install YOLOv8\n","%pip install ultralytics==8.1\n","\n","# Also install this library to record, convert and stream video\n","%pip install ffmpeg==1.4\n","\n","from ultralytics import YOLO, checks\n","checks()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2ftxmU_5-6a"},"outputs":[],"source":["# Feel free to use more libraries\n","import os\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import project_helper as ph\n","from importlib import reload"]},{"cell_type":"code","source":["# Use it if you modify project_helper.py\n","# reload(ph)"],"metadata":{"id":"QYV-vqxWyOFO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aOAH8CvTzSK"},"source":["## Datasets and annotation type in Object Detection\n","\n","In YOLO object detection, the annotation format typically used for training is called the **Darknet annotation format**.\n","\n","The Darknet annotation format consists of **a text file for each image** in your training dataset. Each text file contains lines that describe the objects (bounding boxes) present in the corresponding image. Each line in the annotation file follows a specific format as shown in following figure.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1C4skRQnV2b9CA0ATj5KjNY-0jwCNnIE9\" width=700, align=\"center\"/>\n","\n","\n","`<object-class>`: This is an integer representing the class label or ID of the object detected in the image. Class IDs typically start from 0 and increase for each unique object class.\n","\n","`<x>`: The x-coordinate of the center of the bounding box, normalized to the width of the image. This means that 0 represents the left edge of the image, and 1 represents the right edge.\n","\n","`<y>`: The y-coordinate of the center of the bounding box, normalized to the height of the image. 0 represents the top edge of the image, and 1 represents the bottom edge.\n","\n","`<width>`: The width of the bounding box, also normalized to the width of the image.\n","\n","`<height>`: The height of the bounding box, normalized to the height of the image.\n","\n","Here's an example of what an entry in a Darknet annotation file might look like (note that numbers are separated by single spaces):\n","\n","```\n","0 0.31 0.22 0.08 0.77\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyfR5OUBxkX8"},"outputs":[],"source":["# Create a folder in which we'll leave the images\n","# used for inference\n","\n","# Create inference folder\n","!mkdir inference\n","\n","# Create inference/photos folder\n","!mkdir inference/photos\n","\n","# Go to inference folder\n","%cd inference/photos\n","\n","# Download an example photo in this folder\n","!gdown \"1uzWLdk_13EdyOaVxlvIoo3AMSoLXbHJg\" -O example01.jpg\n","\n","# And go back to the project folder\n","%cd ../.."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTN4tk_NOYYH"},"outputs":[],"source":["# Convert the string into a Path object\n","path_object = os.path.join(PROJECT_DIR, \"inference/photos/example01.jpg\")\n","\n","# Display the original photo\n","ph.show(path_object)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzJhTp33xkk7"},"outputs":[],"source":["# Load a pretrained YOLOv8n detection model\n","model = YOLO('yolov8n.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cyb5eZ_MxkzN"},"outputs":[],"source":["# Predict with the model on the image downloaded\n","results = model(source=path_object,\n","                show=False,\n","                conf=0.3,\n","                save=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkkTmM9FrczJ"},"outputs":[],"source":["# As informed in the output cell above, the results\n","# have been saved\n","print(f'Results saved to {results[0].save_dir}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-CVAB1vnOYgL"},"outputs":[],"source":["# Display the results\n","ph.show(results)"]},{"cell_type":"markdown","metadata":{"id":"BiQQDLnNwj7G"},"source":["Explore the 'results' object. You'll find that it has quite a lot of information in it, as you can see in [the documentation](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Boxes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-N2tErxR-nqg"},"outputs":[],"source":["print(results[0].boxes)"]},{"cell_type":"markdown","source":["Once explored, answer the following questions using code (and **extracting the info from 'results'**):"],"metadata":{"id":"rcOWMi0AAlGQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHox7vnstlu5"},"outputs":[],"source":["# How many bounding boxes have your model found?\n","# Of course you know the solution is 5, but I don't want you to just type 5,\n","# you have to extract the number of boxes from 'results'\n","val = len(results[0].boxes)\n","val = results[0].boxes.shape[0]\n","print(f'The number of bounding boxes found are: {val}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVy4VJIDBaGo"},"outputs":[],"source":["# What's the width of the third bounding box in the range [0-1] (relative to\n","# the width of the image). Return the result as a single value, not a tensor or\n","# a numpy array.\n","\n","b3 = 2  # The third bounding box index\n","val = results[0].boxes.xywhn[b3, 2].item()\n","print(f'The width of the third bounding box is: {val}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-RlUmLhFtlxw"},"outputs":[],"source":["# What coordinates (in pixels) has the center of the second bounding box found\n","# in the figure. Return the result as a tuple of values, i.e. \"(928.5, 1486.2)\"\n","# an not as a tensor or a numpy array\n","b2 = 1  # The second bounding box index\n","val = tuple(np.array(results[0].boxes.xywh[b2, :2].cpu()))\n","print(f'The coords of the center of the second bounding box are: {val}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRwYbj2QBaYl"},"outputs":[],"source":["# What is the model's confidence in the result of the first bounding box?\n","# Return the result as a single value, not a tensor or a numpy array.\n","b1 = 0  # The first bounding box index\n","val = results[0].boxes.conf[b1].cpu().item()\n","print(f'The confidence of the first bounding box is are: {val}')"]},{"cell_type":"code","source":["# Ok, now extract the same data asked in the last 4 cells above\n","# but in this case get it from the raw bbox tensor 'data'\n","\n","img_shape = results[0].boxes.orig_shape\n","dt = results[0].boxes.data.cpu()\n","\n","# How many bounding boxes are there in the image?\n","val1 = dt.shape[0]\n","print(f'The number of bounding boxes found are: {val1}')\n","\n","# What's the width of the third bounding box in the range [0-1]?\n","b3 = 2  # The third bounding box index\n","val2 = (dt[b3, 2] - dt[b3, 0]) / img_shape[1]\n","print(f'The width of the third bounding box is: {val2}')\n","\n","# What coordinates (in pixels) has the center of the second bounding box found\n","# in the figure. Return the result as a tuple of values.\n","b2 = 1  # The second bounding box index\n","val3 = (((dt[b2, 2] - dt[b2, 0]) / 2 + dt[b2, 0]).item(),\n","        ((dt[b2, 3] - dt[b2, 1]) / 2 + dt[b2, 1]).item())\n","print(f'The coords of the center of the second bounding box are: {val3}')\n","\n","# What is the model's confidence in the result of the first bounding box?\n","b1 = 0  # The first bounding box index\n","val4 = dt[b1, 4].item()\n","print(f'The confidence of the first bounding box is are: {val4}')"],"metadata":{"id":"91LpQy6R0D0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDcvXel39Fhv"},"source":["You've already seen a usecase of object detection with YOLO. In this case the model has detected persons, chairs and a potted plant. But, what classes can it detect by default?\n","\n","To see the full list of objects detectable by YOLOv8 by default, you just have to check the dataset in which it was trained on: [the COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/#dataset-yaml)\n","\n","You have the coco.yaml file in the folder you have been given, so extract the conversions dict and list with the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5W-gpdmutl3M"},"outputs":[],"source":["# Extract data from yaml file\n","id2class, class2id = ph.load_classes_from_yaml('coco.yaml')"]},{"cell_type":"code","source":["# See the class names found in the image\n","[id2class[int(id)] for id in results[0].boxes.cls.cpu().tolist()]"],"metadata":{"id":"ZCE7Uv-YEf8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most of the times, when using public datasets, we already have it annotated. Lest's do some real example to adapt annotations.\n","\n","First of all go into your kaggle account. Don't you have one? Don't worry. You just have to go to [Kaggle](kaggle.com) and sign in. You can do it directly with your Google account.\n","\n","Once in Kaggle click on _datasets_ on the left side bar. Now in the text box search for \"_face mask detection_\" and download the first dataset found.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=19QYsPd3avFuwR_heB1p7iWGZiyAxRl89\" width=1000, align=\"center\"/>"],"metadata":{"id":"G3xZmkZLUkWA"}},{"cell_type":"markdown","source":["Now unzip the original dataset with the following instruction and leave it a folder called 'dataset' at the current path. Also, make sure you rename the \"_annotations_\" folder to \"_annotations_XML_\"."],"metadata":{"id":"kELs7wOhlphs"}},{"cell_type":"code","source":["# Define the file paths\n","zip_file_path = 'archive.zip'\n","new_folder_path = 'dataset'\n","\n","# Get annotations names\n","original_annotations_path = os.path.join(new_folder_path, 'annotations')\n","renamed_annotations_path = os.path.join(new_folder_path, 'annotations_XML')\n","\n","# Create a new folder if it doesn't exist\n","if not os.path.exists(new_folder_path):\n","    os.makedirs(new_folder_path)\n","\n","    # Unzip the file into the new folder using the !unzip magic command\n","    !unzip -q $zip_file_path -d $new_folder_path\n","\n","    # Rename the 'annotations' folder to 'annotations_XML'\n","    os.rename(original_annotations_path, renamed_annotations_path)"],"metadata":{"id":"jsLjGH0pxj-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As you can see, in your _dataset_ directory you have two folders, one for the images and another for annotations.\n","\n","Let's have a look at some of those images. Note that we're trying to distinguish between three kinds of objects: A face with a mask, a face without a mask, and a face with a mask incorrectly weared."],"metadata":{"id":"EG3D3Vtf7b_L"}},{"cell_type":"code","source":["# Get a list with image filenames\n","img_filenames = sorted(ph.list_files_in_folder(\n","    os.path.join(PROJECT_DIR, \"dataset/images\"),\n","    absolute=True))"],"metadata":{"id":"dtcLq61HgLQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See some of those images\n","for im in (img_filenames[0], img_filenames[1], img_filenames[38]):\n","    print(im)\n","    ph.show(im)"],"metadata":{"id":"-SyExKzPgShq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check for the files in the annotations folder, you will see that all of them are XML files, that's why we've renamed as \"_annotation_XML_\". This usually corresponds to Pascal VOC annotation format."],"metadata":{"id":"4qtQ86B5fz8w"}},{"cell_type":"code","source":["# Get a list with annotation filenames\n","ann_filenames = sorted(ph.list_files_in_folder(\n","    os.path.join(PROJECT_DIR, \"dataset/annotations_XML\")))"],"metadata":{"id":"xayTi_OLrVaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See a XML file with Pascal VOC annotation\n","# Note that the class names are inside the tags <name></name>\n","# inside each <object></object>\n","ann_id = 0\n","ann_path = os.path.join(PROJECT_DIR, \"dataset/annotations_XML\",\n","                       ann_filenames[ann_id])\n","print(ann_path)\n","with open(ann_path) as f:\n","    contents = f.read()\n","    print(contents)"],"metadata":{"id":"LL5o5Imws4Mv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As said before, the annotation files are in Pascal VOC format, so we are going to use the following cells to transform them to YOLO Darknet format."],"metadata":{"id":"XIzRV5FQl7Pa"}},{"cell_type":"code","source":["# Create a new folder for the YOLO Darknet format annotations\n","new_annot_folder = os.path.join(PROJECT_DIR, 'dataset/annotations')\n","if not os.path.exists(new_annot_folder):\n","    os.makedirs(new_annot_folder)"],"metadata":{"id":"99iqMdfVzQCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check what class names there are in the annotations\n","class_names = sorted(ph.get_classes_from_voc(\"dataset/annotations_XML\"),\n","                     reverse=True)\n","print(class_names)"],"metadata":{"id":"7O_CzkgZ1waW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note: In order to correctly follow the next cell, I suggest you take a look at the provided function 'voc_to_yolo'."],"metadata":{"id":"f3-1dnwPsdfc"}},{"cell_type":"code","source":["# Define the lambda function to check if the folder is empty\n","is_folder_empty = lambda folder_path: not os.listdir(folder_path)\n","\n","# If the folder is empty, do the transformations.\n","if is_folder_empty(new_annot_folder):\n","    # Now transform the files from Pascal VOC format to YOLO Darknet format\n","    ph.voc_to_yolo(\"dataset/annotations_XML\", \"dataset/annotations\", class_names)"],"metadata":{"id":"RRaQzrdFQM4W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get a list with annotation filenames\n","ann_filenames = sorted(ph.list_files_in_folder(new_annot_folder))"],"metadata":{"id":"l3p4RHPvx7l2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the next cell you'll check the YOLO Darknet annotation files. I suggest you try to identify the class number for each class name and verify that it is correct."],"metadata":{"id":"TbeqmxoqOap4"}},{"cell_type":"code","source":["# See a txt file with YOLO Darknet annotation\n","ann_id = 0  # (0, 1, 38)\n","with open(os.path.join(PROJECT_DIR, \"dataset/annotations\",\n","                       ann_filenames[ann_id])) as f:\n","    contents = f.read()\n","    print(contents)"],"metadata":{"id":"RA384gy5ClZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seting up the custom dataset structure\n","\n","Now we're going to train a model on this custom dataset to be able to predict the objects in an image.\n","\n","Before training with YOLOv8 you need to have an specific structure of the dataset.\n","\n","```\n","dataset/\n","├── train/\n","│   ├── images/\n","│   │   ├── img1_train.png\n","│   │   └── img2_train.png\n","│   └── labels/\n","│       ├── label1_train.txt\n","│       └── label2_train.txt\n","├── val/\n","│   ├── images/\n","│   │   ├── img1_val.png\n","│   │   └── img2_val.png\n","│   └── labels/\n","│       ├── label1_val.txt\n","│       └── label2_val.txt\n","└── test/\n","    ├── images/\n","    │   ├── img1_test.png\n","    │   └── img2_test.png\n","    └── labels/\n","        ├── label1_test.txt\n","        └── label2_test.txt\n","```\n","\n","So first of all we're going to arrange the dataset so it suits this structure."],"metadata":{"id":"KYGj1XP9eX4f"}},{"cell_type":"code","source":["# Get a list of all image files in the dataset folder\n","img_filenames = sorted(ph.list_files_in_folder(\n","    os.path.join(PROJECT_DIR, \"dataset/images\"),\n","    absolute=True))\n","\n","# Get a list of all annotation files in the dataset folder\n","ann_filenames = sorted(ph.list_files_in_folder(\n","    os.path.join(PROJECT_DIR, \"dataset/annotations\"),\n","    absolute=True))\n","\n","# Pair the elements of the two lists and shuffle\n","paired_list = list(zip(img_filenames, ann_filenames))\n","random.seed(42)\n","random.shuffle(paired_list)\n","\n","# Unzip the pairs back into two lists\n","img_filenames, ann_filenames = zip(*paired_list)\n","\n","# Select the percentage for testing and validation\n","test_percent = 18\n","val_percent = 20\n","test_size = int(len(img_filenames) * test_percent / 100)\n","val_size = int(len(img_filenames) * val_percent / 100)\n","\n","# Select the train, validation and test lists\n","test_img_filenames = img_filenames[:test_size]\n","test_ann_filenames = ann_filenames[:test_size]\n","val_img_filenames = img_filenames[test_size:test_size+val_size]\n","val_ann_filenames = ann_filenames[test_size:test_size+val_size]\n","train_img_filenames = img_filenames[test_size+val_size:]\n","train_ann_filenames = ann_filenames[test_size+val_size:]"],"metadata":{"id":"muQMPeYHYxTo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a new folder called \"data\"\n","!mkdir data\n","\n","# Create the subfolders 'train', 'val' and 'test'\n","!mkdir data/train\n","!mkdir data/val\n","!mkdir data/test\n","\n","# Create the subfolders of images and labels\n","!mkdir data/train/images\n","!mkdir data/train/labels\n","!mkdir data/val/images\n","!mkdir data/val/labels\n","!mkdir data/test/images\n","!mkdir data/test/labels"],"metadata":{"id":"2A9gFzqaBd9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copy files to the correspondig folders (it may take a while)\n","ph.copy_files(test_img_filenames, 'data/test/images')\n","ph.copy_files(test_ann_filenames, 'data/test/labels')\n","ph.copy_files(val_img_filenames, 'data/val/images')\n","ph.copy_files(val_ann_filenames, 'data/val/labels')\n","ph.copy_files(train_img_filenames, 'data/train/images')\n","ph.copy_files(train_ann_filenames, 'data/train/labels')"],"metadata":{"id":"wi57YN8_BeKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training on custom dataset"],"metadata":{"id":"ORJkELAagaFf"}},{"cell_type":"markdown","source":["When training with YOLOv8 you'll need to provide a yaml file similar to the one seen before [here](https://docs.ultralytics.com/datasets/detect/coco/#dataset-yaml)\n","\n","Create this file with all the necessary contents to do the training. You can skip the '_download_' section. The name of this file must be '**dataset_mask_1.yaml**'. Save the file at the project directory."],"metadata":{"id":"ODLJ2RzIWYlZ"}},{"cell_type":"code","source":["def create_dataset_txt(main_path, train, val, test, class_list):\n","    \"\"\"\n","    Creates a text file describing a dataset with paths and class names.\n","\n","    Parameters:\n","    main_path (str): Absolute path to the main dataset directory.\n","    train (str): Relative path from main_path to the training set directory.\n","    val (str): Relative path from main_path to the validation set directory.\n","    test (str): Relative path from main_path to the test set directory.\n","    class_list (list of str): List of class names.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","\n","    # Convert the class list to a string\n","    class_list_str = ', '.join(class_list)\n","\n","    # Prepare the content to be written in the text file\n","    content = f\"\"\"\n","# Dataset path\n","\n","# You can use absolute path (recommended)\n","path: {main_path}\n","\n","# Train/val/test sets as 1) dir: path/to/imgs,\n","#                        2) file: path/to/imgs.txt, or\n","#                        3) list: [path/to/imgs1, path/to/imgs2, ..]\n","\n","# Here references must be relative to path\n","train: {train}\n","val: {val}\n","test: {test}\n","\n","# Class Names\n","names: [{class_list_str}]\n","    \"\"\"\n","\n","    # Write the content to a text file in the current directory\n","    with open('dataset_mask_1.yaml', 'w') as file:\n","        file.write(content)\n","\n","create_dataset_txt(PROJECT_DIR,\n","                   'data/train/images',\n","                   'data/val/images',\n","                   'data/test/images',\n","                   class_names)"],"metadata":{"id":"yqdTXTJrlCAY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# So it's time to train. Training is so easy as shown in the following\n","# line. You can tweak the parameters shown in the documentation, of course.\n","model.train(data='dataset_mask_1.yaml', epochs=30)"],"metadata":{"id":"0GTnlPXElCL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the folder where the info of the last training has been saved\n","tr_folder = sorted([name for name in os.listdir('runs/detect')\n","                   if os.path.isdir(os.path.join('runs/detect', name)) and\n","                   name.startswith('train')])[-1]\n","\n","print(tr_folder)"],"metadata":{"id":"kj9_9dd1g5Us"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See the metrics. Explore and understand what you see.\n","%load_ext tensorboard\n","%tensorboard --logdir {'runs/detect/' + tr_folder}"],"metadata":{"id":"ZgsyR-wSlCP5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Object detection\n","\n","Let's see how the trained model works."],"metadata":{"id":"wIDkwBrkoPO3"}},{"cell_type":"code","source":["# Get the best model\n","model = YOLO(f'runs/detect/{tr_folder}/weights/best.pt')"],"metadata":{"id":"CuV6vlPDvkOz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get some test images\n","path = 'data/test/images'\n","test_images = sorted(ph.list_files_in_folder(path, absolute=True))[2:11]"],"metadata":{"id":"W7K0yEc7wNqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate inference\n","results = model(source=test_images,\n","                show=False,\n","                conf=0.3,\n","                save=True)"],"metadata":{"id":"UnBgGm5YohjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show some results\n","for k in range(len(results)):\n","    ph.show(results[k])"],"metadata":{"id":"LnrgJGJhTiXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The way to test the model is with the test dataset.\n","\n","# Test the model with split='test'. Dataset and settings are remembered.\n","metrics = model.val(split='test')\n","\n","print(metrics.box.map)    # map50-95\n","print(metrics.box.map50)  # map50\n","print(metrics.box.map75)  # map75\n","print(metrics.box.maps)   # a list contains map50-95 of each category"],"metadata":{"id":"QyJCUw5Ra4XI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## K-fold cross validation\n","\n","Once you've been able to do a training with your custom dataset, it has arrived the moment to go one step further.\n","\n","You may have already realized that the strategy of configuring a folder structure with 'train', 'val' and 'test' is quite rigid in the sense that it is not agile to change files from one folder to another.\n","\n","There must be a more agile way of managing the files that will be part of each set, and in fact there is. Find out!\n","\n","In this section you have to set up a training based on 8-fold cross validation, do this training and report the average mAP50 metric. You should do this using the files intended for 'train' and 'val'.\n","\n","Naturally, the management of the files that will make up each set has to be dynamic, and it is not acceptable for these files to change their location on disk.\n","\n","Note: It's enough that you train for 5 epochs."],"metadata":{"id":"RmLcpzYeVXpN"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold"],"metadata":{"id":"A4ZCBhChlCTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load a pretrained YOLOv8n detection model\n","model = YOLO('yolov8n.pt')"],"metadata":{"id":"qBH5pQZ4l0md"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def kfold_cv(length, k, test_percent):\n","    \"\"\"\n","    Generates indices for test data and k-fold cross validation on a dataset.\n","\n","    Args:\n","    length (int): Total number of images in the dataset.\n","    k (int): Number of folds for k-fold cross validation.\n","    test_percent (float): Percentage of the dataset to be used for testing.\n","\n","    Returns:\n","    tuple:\n","        - test_ids (list): A list of indices for test data.\n","        - kfold_generator (generator): A generator that yields training and\n","            validation indices for each fold.\n","    \"\"\"\n","    # Calculate the number of test images\n","    num_test_images = int(length * test_percent / 100)\n","\n","    # Randomly select test indices\n","    all_indices = np.arange(length)\n","    np.random.shuffle(all_indices)\n","    test_ids = list(all_indices[:num_test_images])\n","\n","    # Remaining indices for k-fold\n","    remaining_indices = all_indices[num_test_images:]\n","\n","    # K-Fold Cross Validation\n","    kf = KFold(n_splits=k)\n","    kfold_generator = ((list(remaining_indices[train_index]),\n","                        list(remaining_indices[val_index])) for train_index,\n","                        val_index in kf.split(remaining_indices))\n","\n","    return test_ids, kfold_generator"],"metadata":{"id":"iA5mvCYJsYsQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def select_strings(strings, indices, pre='', post=''):\n","    \"\"\"\n","    Selects and returns strings from a list based on a list of indices.\n","\n","    Args:\n","    strings (list of str): The list of strings.\n","    indices (list of int): The list of indices to select strings.\n","    pre (string): String to add before the indexed string.\n","    post (string): String to add after the indexed string.\n","\n","    Returns:\n","    list of str: The list of selected strings.\n","\n","    Example:\n","    select_strings(['John', 'Tom', 'Jack', 'Sam'], [0, 3], pre='Hello ')\n","    >>> ['Hello John', 'Hello Sam']\n","    \"\"\"\n","    return [pre + strings[i] + post for i in indices if i < len(strings)]"],"metadata":{"id":"lYH-P6JDtLkE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_text_file(strings, folder_path=None, file_name=\"output.txt\"):\n","    \"\"\"\n","    Creates a text file with each string from the list on a separate line\n","    in UTF-8 format.\n","\n","    Args:\n","    strings (list of str): The list of strings to be written to the file.\n","    folder_path (str, optional): The path to the folder where the file will\n","        be saved. If not provided, the file is saved in the current directory.\n","    file_name (str, optional): The name of the file to be created. Defaults\n","        to 'output.txt'.\n","    \"\"\"\n","    # If a folder path is provided, use it; otherwise, use the current\n","    # directory\n","    if folder_path:\n","        full_path = os.path.join(folder_path, file_name)\n","    else:\n","        full_path = file_name\n","\n","    # Writing the strings to the file in UTF-8 format\n","    with open(full_path, 'w', encoding='utf-8') as file:\n","        for string in strings:\n","            file.write(string + '\\n')\n"],"metadata":{"id":"cSeG6nvxt3Mq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Before executing next cell make sure you have your file\n","# 'dataset_mask_2.yaml' properly configured. Specifically, it must\n","# point to the files `test.txt`, `val.txt` and `train.txt`.\n","\n","# Get the dataset folder\n","dataset_path = os.path.join(PROJECT_DIR, 'data')\n","\n","# List all image files\n","all_files = [f for f in ph.list_files_in_folder(dataset_path,\n","    absolute=True, recursivity=True) if str(f).endswith('.png')]\n","\n","# Select parameters in k-fold cross validation\n","k = 8\n","test_percent = 18  # Percentage of data for testing\n","test_ids, kfold_gen = kfold_cv(len(all_files), k, test_percent)\n","\n","# Get paths list for testing and create text file\n","test_paths = select_strings(all_files, test_ids)\n","create_text_file(test_paths, dataset_path, 'test.txt')"],"metadata":{"id":"4_f7vdVvu80M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the models\n","metric = []\n","for train_ids, val_ids in kfold_gen:\n","    # Get training and validation images paths\n","    train_paths = select_strings(all_files, train_ids)\n","    val_paths = select_strings(all_files, val_ids)\n","\n","    # Create text files with the lists of images paths\n","    create_text_file(train_paths, dataset_path, 'train.txt')\n","    create_text_file(val_paths, dataset_path, 'val.txt')\n","\n","    # Train the model\n","    model.train(data='dataset_mask_2.yaml', epochs=5, resume=False)\n","    metric.append(model.metrics.results_dict['metrics/mAP50(B)'])"],"metadata":{"id":"bOwMVDFBz1Y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See the reported metric of each model\n","print(f'Metric mAP50: {metric}')\n","\n","# See the average:\n","print(f'Average of all mAP50: {np.mean(metric)}')"],"metadata":{"id":"0oVTXCvf0MYd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Squat counter\n","\n","Now that you have been able to practice with YOLOv8, we are going to face the last exercise. You must do this last exercise **locally on your laptop**. You don't have to worry about the GPU as no training is required.\n","\n","The exercise consists of creating a **squat counter application**. This application must be able to receive as input both a video file or directly the webcam input. Only one person must appear in the image. The application must be able to distinguish whether the person is up (standing) or down (squatting) taking as reference the angles between the hip-knee and knee-ankle vectors of the two legs. Additionally, the application must be able to count how many squats are done in the video.\n","\n","You are provided with a video in which a person appears doing squats and some pieces of code that could be usefull. **The following chunks of code aren't mandatory, they are just suggestions**\n","\n","It's highly recommended to create a virtual environment with python 3.10 and ultralytics."],"metadata":{"id":"8rRNSxaLoAGJ"}},{"cell_type":"code","source":["# Fully functional local version of the squat counter\n","# Runs on a virtual environment with Python 3.10 and ultralytics 8.0.201\n","\n","##################################################\n","# Libraries\n","##################################################\n","\n","import math\n","from collections import deque\n","import cv2\n","import numpy as np\n","from PIL import Image, ImageDraw\n","from ultralytics import YOLO\n","\n","\n","##################################################\n","# Global variables and general set up\n","##################################################\n","\n","# Body parts ordered as indicated in keypoints\n","idx2bparts = [\"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\",\n","    \"Left Shoulder\", \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\",\n","    \"Left Wrist\", \"Right Wrist\", \"Left Hip\", \"Right Hip\", \"Left Knee\",\n","    \"Right Knee\", \"Left Ankle\", \"Right Ankle\"]\n","\n","# Index of body parts\n","bparts2idx = {key: ix for ix, key in enumerate(idx2bparts)}\n","\n","# State and squat count\n","STATE = 'UP'\n","COUNT = 0\n","state_stack = deque(maxlen=6)\n","CHECK = True  # Used for debugging\n","ONE_IMAGE = False\n","\n","# Load the Yolov8 model\n","model = YOLO('src/models/yolov8s-pose.pt')\n","\n","# Open the video file\n","source = 0\n","# source = \"src/inference/videos/MySquats.mp4\"\n","\n","\n","##################################################\n","# Helper functions\n","##################################################\n","\n","def add_annotations(frame):\n","    \"\"\"\n","    Add state (up/down) and squats count (number) to the image.\n","\n","    Args:\n","        frame (numpy array): Current frame captured\n","\n","    Returns:\n","        frame with added text\n","    \"\"\"\n","    # Display state and count on the image\n","    state_text = f\"State: {STATE}\"\n","    count_text = f\"Count: {COUNT}\"\n","\n","    # Define the position and font settings for the text\n","    text_position1 = (10, 30)\n","    text_position2 = (10, 60)\n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","    font_scale = 0.7\n","    green = (0, 255, 0)\n","    red = (0, 0, 255)\n","    font_color = green if STATE == 'UP' else red\n","    font_thickness = 2\n","\n","    frame_with_text = frame.copy()\n","    cv2.putText(frame_with_text, state_text, text_position1, font,\n","                font_scale, font_color, font_thickness)\n","    cv2.putText(frame_with_text, count_text, text_position2, font,\n","                font_scale, green, font_thickness)\n","\n","    return frame_with_text\n","\n","\n","def legs_angles(left, right, verbose=False):\n","    \"\"\"\n","    It calculates the minimum angle that make up the vector hip-knee with\n","    the vector knee-ankle in each leg. The inputs are numpy arrays with\n","    shape 3x2 (3 points x 2 coordinates) and the output is a numpy array\n","    of shape [2,] with each angle in degrees.\n","\n","    Args:\n","        left (numpy array): Coordinates of joints hip, knee and ankle of\n","            the left leg. The matrix has the following shape:\n","            [x hip  , y hip  ]\n","            [x knee , y knee ]\n","            [x ankle, y ankle]\n","        right (numpy array): Coordinates of joints hip, knee and ankle of\n","            the right leg. The matrix has the same shape as 'left'\n","        verbose (bool, optional): Print info. Defaults to False.\n","\n","    Returns:\n","        A numpy array with shape [2,] with the angles of the two legs in\n","            degrees.\n","    \"\"\"\n","\n","    angles = []\n","\n","    for v in [left, right]:\n","        # Define the coordinates of three points (x1, y1), (x2, y2), and (x3, y3)\n","        x1, y1 = v[0, 0], v[0, 1]\n","        x2, y2 = v[1, 0], v[1, 1]\n","        x3, y3 = v[2, 0], v[2, 1]\n","\n","        # Calculate the vectors from p2 to p1 and from p2 to p3\n","        vector1 = (x1 - x2, y1 - y2)\n","        vector2 = (x3 - x2, y3 - y2)\n","\n","        # Calculate the dot product of the vectors\n","        dot_product = vector1[0] * vector2[0] + vector1[1] * vector2[1]\n","\n","        # Calculate the magnitudes of the vectors\n","        magnitude1 = math.sqrt(vector1[0]**2 + vector1[1]**2)\n","        magnitude2 = math.sqrt(vector2[0]**2 + vector2[1]**2)\n","\n","        # Calculate the cosine of the angle using the dot product\n","        cosine_theta = dot_product / (magnitude1 * magnitude2)\n","\n","        # Calculate the angle in radians\n","        theta_radians = math.acos(max(-1, min(cosine_theta, 1)))\n","\n","        # Convert the angle from radians to degrees\n","        theta_degrees = math.degrees(theta_radians)\n","\n","        # Append the angles to the list\n","        angles.append(theta_degrees)\n","\n","        if verbose:\n","            print((f\"The angle in the knee (triangle knee-hip-ankle) is \"\n","                   f\"{theta_degrees:.2f} degrees.\"))\n","\n","    return np.array(angles)\n","\n","\n","def get_legs_coords(kpts):\n","    \"\"\"\n","    It gets the keypoints of the result object and extract those from\n","    hip, knee and ankle of left and right legs. The outputs are np arrays\n","    with the coordinates x, y and the confidence value\n","\n","    Args:\n","        kpts (ultralytics keypoints): Keypoints object from the Result\n","            object in a pose estimation.\n","\n","    Returns:\n","        left_leg_coords (numpy array): 3x3 numpy array with the coordinates\n","            (x, y, confidence) of the left hip, left knee and left ankle\n","            in the image\n","        left_leg_coords (numpy array): 3x3 numpy array with the coordinates\n","            (x, y, confidence) of the left hip, left knee and left ankle\n","            in the image\n","    \"\"\"\n","    # Indices of left and right hip, knee and ankle\n","    left_leg = [11, 13, 15]\n","    right_leg = [12, 14, 16]\n","\n","    # Left leg\n","    left_leg_coords = kpts.data[0, left_leg, :].cpu().numpy()\n","\n","    # Right leg\n","    right_leg_coords = kpts.data[0, right_leg, :].cpu().numpy()\n","\n","    return left_leg_coords, right_leg_coords\n","\n","\n","def extract(result):\n","    \"\"\"\n","    Explore the Results object of Ultralytics for pose estimation\n","\n","    This is just a helper function in the sense it could help how to explore\n","    some fields in the Results objects. You won't really need this function\n","    to implement any functionality.\n","\n","    Args:\n","        result (Ultralytics Results): Object extracted from a Results generator\n","            or a Results list.\n","\n","    Returns:\n","        None. It prints out some info contained in the input object.\n","    \"\"\"\n","    # Body parts ordered as indicated in keypoints\n","    idx2bparts = [\"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\",\n","        \"Left Shoulder\", \"Right Shoulder\", \"Left Elbow\", \"Right Elbow\",\n","        \"Left Wrist\", \"Right Wrist\", \"Left Hip\", \"Right Hip\", \"Left Knee\",\n","        \"Right Knee\", \"Left Ankle\", \"Right Ankle\"]\n","\n","    # Index of body parts\n","    bparts2idx = {key: ix for ix, key in enumerate(idx2bparts)}\n","\n","    # Process result generator\n","    output_str = ''\n","    for ix, r in enumerate(result):\n","        names = r.names\n","\n","        # Boxes object for bbox outputs\n","        box = r.boxes\n","        output_str += \"\\n\\nBOXES\\n-----\\n\"\n","        output_str += f\"Box {ix}\\n\"\n","        output_str += f\"Name of object: {names[int(box.cls.item())]}\\n\"\n","        output_str += f\"Normalized coordinates of the box (xyxy): {box.xyxyn}\\n\"\n","        output_str += f\"Confidence of detection: {box.conf.item()}\\n\"\n","\n","        kpts = r.keypoints  # Keypoints object for pose outputs\n","        output_str += \"\\n\\nKEYPOINTS\\n---------\\n\"\n","        output_str += \"Coordinates normalized\\n\"\n","        for kp in kpts:\n","            output_str += f\"Nose: {kp.xyn[0, bparts2idx['Nose']]}\\n\"\n","            output_str += f\"Left Shoulder: {kp.xyn[0, bparts2idx['Left Shoulder']]}\\n\"\n","            output_str += f\"Right Shoulder: {kp.xyn[0, bparts2idx['Right Shoulder']]}\\n\"\n","            output_str += f\"Left Hip: {kp.xyn[0, bparts2idx['Left Hip']]}\\n\"\n","            output_str += f\"Right Hip: {kp.xyn[0, bparts2idx['Right Hip']]}\\n\"\n","            output_str += f\"Left Knee: {kp.xyn[0, bparts2idx['Left Knee']]}\\n\"\n","            output_str += f\"Right Knee: {kp.xyn[0, bparts2idx['Right Knee']]}\\n\"\n","            output_str += f\"Left Ankle: {kp.xyn[0, bparts2idx['Left Ankle']]}\\n\"\n","            output_str += f\"Right Ankle: {kp.xyn[0, bparts2idx['Right Ankle']]}\\n\"\n","\n","        print(output_str)\n","\n","        # You could also explore masks and probs\n","\n","        # Masks object for segmentation masks outputs\n","        # masks = result.masks\n","\n","        # Probs object for classification outputs\n","        # probs = result.probs\n","\n","\n","def evaluate_position(result, limit_conf=0.3, verbose=False):\n","    \"\"\"\n","    Evaluate position of the body in the image\n","\n","    It updates the global variables STATE (UP or DOWN) and the number\n","    of squats done (COUNT)\n","\n","    Args:\n","        result (Ultralytics Results): Results object from Ultralytics. It\n","            contains all the data of the pose estimation.\n","        limit_conf (float, optional): It's the limiting confidence. Greater\n","            confidences in (all) points estimation will be considered,\n","            otherwise they will be descarted. Defaults to 0.3.\n","        verbose (bool, optional): Print info. Defaults to False.\n","    \"\"\"\n","\n","    # Global variables\n","    global COUNT\n","    global STATE\n","    global state_stack\n","\n","    # Loop through Ultralytics Results\n","    for r in result:\n","\n","        # Get bounding boxes\n","        box = r.boxes\n","        if r.names[int(box.cls.item())] != 'person':\n","            print(\"First box is not a person\")\n","            break\n","\n","        # Get keypoints\n","        kpts = r.keypoints  # Keypoints object for pose outputs\n","\n","        # Get coordinates of the joints of the left and right legs\n","        left_coords, right_coords = get_legs_coords(kpts)\n","\n","        # Check for confidences\n","        if (left_coords[:, 2] > limit_conf).all() and (right_coords[:, 2] > limit_conf).all():\n","\n","            # Calculate the minimum angle in both legs\n","            angles = legs_angles(left_coords[:, :2], right_coords[:, :2])\n","\n","            # Legs bent or stretched\n","            if (angles < 120).all() and STATE=='UP':\n","                STATE = 'DOWN'\n","            elif (angles > 150).all() and STATE=='DOWN':\n","                STATE = 'UP'\n","\n","            # Update stack of states and count\n","            state_stack.append(STATE)\n","            if len(state_stack)==6:\n","                if state_stack == deque(\n","                    ['DOWN', 'DOWN', 'DOWN', 'UP', 'UP', 'UP']):\n","                    COUNT += 1\n","\n","    # Show info if required\n","    if verbose:\n","        print(f\"State: {STATE}\")\n","        print(f\"Count: {COUNT}\")\n","\n","\n","def draw_grid_on_image(img, grid_size=(10, 10)):\n","    \"\"\"\n","    Function to draw a grid on an image.\n","    \"\"\"\n","    draw = ImageDraw.Draw(img)\n","\n","    # Get image dimensions\n","    img_width, img_height = img.size\n","\n","    # Calculate cell dimensions\n","    cell_width = img_width / grid_size[0]\n","    cell_height = img_height / grid_size[1]\n","\n","    # Calculate vertical line positions\n","    vertical_lines = [(i * cell_width, 0, i * cell_width, img_height) for\n","                      i in range(grid_size[0] + 1)]\n","\n","    # Calculate horizontal line positions\n","    horizontal_lines = [(0, i * cell_height, img_width, i * cell_height) for\n","                        i in range(grid_size[1] + 1)]\n","\n","    # Draw all lines\n","    for line in vertical_lines + horizontal_lines:\n","        draw.line(line, fill=\"black\")\n","\n","    # Return the image with the grid\n","    return img\n","\n","\n","##################################################\n","# Main program\n","##################################################\n","\n","# Select source\n","cap = cv2.VideoCapture(source)\n","stream = True  # If stream=True the output is a generator\n","               # otherwise it's a list\n","\n","# Loop through the video frames\n","cont = 0\n","while cap.isOpened():\n","    # Read a frame from the video\n","    success, frame = cap.read()\n","\n","    # If the frame is empty, break the loop\n","    if not success:\n","        break\n","\n","    # Perform pose estimation on this single frame\n","    results = model(source=frame,\n","                    show=True,\n","                    conf=0.3,  # Confidence greater than\n","                    save=False,\n","                    stream=stream)  # Create a generator instead of a list\n","\n","    # Extract data from results\n","    if not stream:  # En caso de que stream=False\n","        r = results[0]\n","    else:\n","        r = next(results)\n","\n","    if cont == 0:\n","        if ONE_IMAGE:\n","            cv2.destroyWindow('image0.jpg')\n","        else:\n","            cv2.setWindowTitle('image0.jpg', 'YoloV8 Results')\n","\n","    # Convert to image\n","    if CHECK:\n","        im = draw_grid_on_image(Image.fromarray(r.plot()[..., ::-1]))\n","        im.show()\n","\n","    # Evaluate position\n","    evaluate_position(r)\n","\n","    frame_with_text = add_annotations(frame)\n","\n","    # Display the annotated frame\n","    cv2.imshow('Squat Counter Window', frame_with_text)\n","\n","    # Check for user input to break the loop (e.g., press 'q' to exit)\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","    # Increment frame counter\n","    cont +=1\n","\n","# Release the video capture object and close all windows\n","cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"ecQYRVGIFTUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assesment\n","\n","Deliver this notebook **with the results of the executed cells**. You also must include **your squat counter app as a main.py file** and attach any other file needed to execute the code (helper functions).\n","\n","I hope you've learned a lot and you've enjoyed this final lab."],"metadata":{"id":"Ozs7eP2Aq8wK"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["vmSsF_fXZQYW"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}